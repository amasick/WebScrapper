{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FRsVqIfDjSMA-jUFkCrx2mrEoW4bK1vv",
      "authorship_tag": "ABX9TyMoF5ajSonB39N0jjz4p6u5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amasick/WebScrapper/blob/main/Schools_Details.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# CBSE Schools Scraper Documentation\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This script is designed to scrape school details from the CBSE (Central Board of Secondary Education) website. It targets schools located outside India and extracts information such as school name, email, address, PIN code, management, and a \"Read More\" link.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "The script requires the following Python libraries to be installed:\n",
        "\n",
        "- `requests`: For making HTTP requests.\n",
        "- `BeautifulSoup` (from `bs4` package): For parsing HTML content.\n",
        "- `pandas`: For handling and exporting data to Excel.\n",
        "- `re`: For regular expressions.\n",
        "\n",
        "You can install these dependencies using the following:\n",
        "\n",
        "```bash\n",
        "pip install requests beautifulsoup4 pandas\n",
        "```\n",
        "\n",
        "## Script Overview\n",
        "\n",
        "### Function: `scrape_school_details(catbox_div)`\n",
        "\n",
        "This function takes a BeautifulSoup object representing a `div` element with the class 'catbox', which contains details about a school. It extracts the school name, email, address, PIN code, management, and the \"Read More\" link.\n",
        "\n",
        "#### Parameters:\n",
        "- `catbox_div` (BeautifulSoup object): The `div` element containing school details.\n",
        "\n",
        "#### Returns:\n",
        "- `dict`: A dictionary containing school details.\n",
        "\n",
        "### Function: `scrape_all_schools(url)`\n",
        "\n",
        "This function is responsible for scraping all schools' data from a given base URL. It iterates through multiple pages, collects data from each page using `scrape_school_details`, and aggregates the results.\n",
        "\n",
        "#### Parameters:\n",
        "- `url` (str): The base URL for the CBSE schools outside India.\n",
        "\n",
        "#### Returns:\n",
        "- `list`: A list of dictionaries, where each dictionary represents the details of a school.\n",
        "\n",
        "### Function: `save_to_excel(data, output_file='Outside_India.xlsx')`\n",
        "\n",
        "This function takes the scraped school data and saves it to an Excel file.\n",
        "\n",
        "#### Parameters:\n",
        "- `data` (list of dict): List of dictionaries containing school details.\n",
        "- `output_file` (str, optional): The name of the output Excel file. Default is 'Outside_India.xlsx'.\n",
        "\n",
        "### Main Section: `if __name__ == \"__main__\":`\n",
        "\n",
        "The main section of the script sets the base URL, initializes an empty list (`all_schools_data`) to store scraped data, and iterates through pages to scrape school details. It then calls the `save_to_excel` function to export the data to an Excel file.\n",
        "\n",
        "## Execution\n",
        "\n",
        "To run the script, ensure you have the required dependencies installed and execute it using:\n",
        "\n",
        "```bash\n",
        "python script_name.py\n",
        "```\n",
        "\n",
        "Make sure to replace `script_name.py` with the actual name of your Python script.\n",
        "\n",
        "## Customization\n",
        "\n",
        "- **Base URL**: You can modify the `base_url` variable to target a different location or category of schools.\n",
        "\n",
        "- **Number of Pages**: Adjust the loop range in the main section (`for page_number in range(1, 5):`) based on the actual number of pages.\n",
        "\n",
        "- **Output File**: You can change the default output file name in the `save_to_excel` function if needed.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This script provides a scalable solution for scraping CBSE school details, and you can customize it to meet specific requirements."
      ],
      "metadata": {
        "id": "UOjefXGQyp3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OgRLcG6hPyA",
        "outputId": "32472e3e-aa5d-4d07-ad90-e3d75fcb314d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to Outside_India.xlsx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "def scrape_school_details(catbox_div):\n",
        "    school_name = catbox_div.find('h2').find('a').text.strip()\n",
        "\n",
        "    # Find the paragraph containing school details\n",
        "    school_details_paragraph = catbox_div.find('p')\n",
        "\n",
        "    # Check if the paragraph is found before accessing its content\n",
        "    if school_details_paragraph:\n",
        "        # Use regex to extract email\n",
        "        email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', school_details_paragraph.text)\n",
        "        email = email_match.group(0).strip() if email_match else None\n",
        "\n",
        "\n",
        "        # Use regex to extract address\n",
        "        address_match = re.search(r'Address of the school is:(.*?)PIN Code:', school_details_paragraph.text)\n",
        "        address = address_match.group(1).strip() if address_match else None\n",
        "        # print(address)\n",
        "\n",
        "        # Use regex to extract PIN code\n",
        "        pin_code_match = re.search(r'PIN Code:\\s*([^<.]+)', school_details_paragraph.text)\n",
        "        pin_code = pin_code_match.group(1).strip() if pin_code_match else None\n",
        "\n",
        "        # Use regex to extract management\n",
        "        management_match = re.search(r'The school is being managed by\\s*([^<]+)\\.', school_details_paragraph.text)\n",
        "        management = management_match.group(1).strip() if management_match else None\n",
        "\n",
        "        read_more_link = school_details_paragraph.find('a', class_='link')['href']\n",
        "\n",
        "        return {\n",
        "            'School Name': school_name,\n",
        "            'Email': email,\n",
        "            'Address': address,\n",
        "            'PIN Code': pin_code,\n",
        "            'Management': management,\n",
        "            'Read More Link': read_more_link\n",
        "        }\n",
        "    else:\n",
        "        print(\"School details paragraph not found.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def scrape_all_schools(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0'}\n",
        "\n",
        "    # Make the GET request with custom headers\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        catbox_divs = soup.find_all('div', class_='catbox')\n",
        "\n",
        "        all_schools_data = []\n",
        "        for catbox_div in catbox_divs:\n",
        "            school_data = scrape_school_details(catbox_div)\n",
        "            all_schools_data.append(school_data)\n",
        "\n",
        "        return all_schools_data\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
        "        return None\n",
        "def save_to_excel(data, output_file='TN_Schools.xlsx'):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://www.cbseschool.org/location/outside-india/page/\"\n",
        "    all_schools_data = []\n",
        "\n",
        "    for page_number in range(1,5):  # Assuming you have 24 pages\n",
        "        url = f\"{base_url}{page_number}\"\n",
        "        page_schools_data = scrape_all_schools(url)\n",
        "\n",
        "        if page_schools_data:\n",
        "            all_schools_data.extend(page_schools_data)\n",
        "\n",
        "    if all_schools_data:\n",
        "        save_to_excel(all_schools_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Outside India Schools**"
      ],
      "metadata": {
        "id": "0ZVXL8xS-CAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "def scrape_school_details(catbox_div):\n",
        "    school_name = catbox_div.find('h2').find('a').text.strip()\n",
        "\n",
        "    # Find the paragraph containing school details\n",
        "    school_details_paragraph = catbox_div.find('p')\n",
        "\n",
        "    # Check if the paragraph is found before accessing its content\n",
        "    if school_details_paragraph:\n",
        "        # Use regex to extract email\n",
        "        email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', school_details_paragraph.text)\n",
        "        email = email_match.group(0).strip() if email_match else None\n",
        "\n",
        "\n",
        "        # Use regex to extract address\n",
        "        address_match = re.search(r'Address of the school is:(.*?)PIN Code:', school_details_paragraph.text)\n",
        "        address = address_match.group(1).strip() if address_match else None\n",
        "        # print(address)\n",
        "\n",
        "        # Use regex to extract PIN code\n",
        "        pin_code_match = re.search(r'PIN Code:\\s*([^<.]+)', school_details_paragraph.text)\n",
        "        pin_code = pin_code_match.group(1).strip() if pin_code_match else None\n",
        "\n",
        "        # Use regex to extract management\n",
        "        management_match = re.search(r'The school is being managed by\\s*([^<]+)\\.', school_details_paragraph.text)\n",
        "        management = management_match.group(1).strip() if management_match else None\n",
        "\n",
        "        read_more_link = school_details_paragraph.find('a', class_='link')['href']\n",
        "\n",
        "        return {\n",
        "            'School Name': school_name,\n",
        "            'Email': email,\n",
        "            'Address': address,\n",
        "            'PIN Code': pin_code,\n",
        "            'Management': management,\n",
        "            'Read More Link': read_more_link\n",
        "        }\n",
        "    else:\n",
        "        print(\"School details paragraph not found.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def scrape_all_schools(url):\n",
        "    # headers = {\n",
        "    #     'User-Agent': 'Thunder Client (https://www.thunderclient.com)',\n",
        "    #     'Accept': '/'\n",
        "    # }\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0'}\n",
        "\n",
        "\n",
        "    # Make the GET request with custom headers\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        catbox_divs = soup.find_all('div', class_='catbox')\n",
        "\n",
        "        all_schools_data = []\n",
        "        for catbox_div in catbox_divs:\n",
        "            school_data = scrape_school_details(catbox_div)\n",
        "            all_schools_data.append(school_data)\n",
        "\n",
        "        return all_schools_data\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def save_to_excel(data, output_file='all_schools_details12.xlsx'):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://www.cbseschool.org/location/outside-india/\"\n",
        "    all_schools_data = []\n",
        "\n",
        "    for page_number in range(1, 25):  # Assuming you have 24 pages\n",
        "        url = f\"{base_url}{page_number}\"\n",
        "        page_schools_data = scrape_all_schools(url)\n",
        "\n",
        "        if page_schools_data:\n",
        "            all_schools_data.extend(page_schools_data)\n",
        "\n",
        "    if all_schools_data:\n",
        "        save_to_excel(all_schools_data)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qguXOLRB-NZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Function to scrape details of a school from a given li element\n",
        "def scrape_school_details(li_element):\n",
        "    # Find the div containing school details\n",
        "    school_details_div = li_element.find('div', class_='edu-school-detlist-container')\n",
        "\n",
        "    # Check if school details are found\n",
        "    if school_details_div:\n",
        "        # Extracting the school name\n",
        "        school_name = school_details_div.find('h2', class_='edu-school-det-heading').text.strip()\n",
        "\n",
        "        # Initializing a dictionary to store school details\n",
        "        details = {'School Name': school_name}\n",
        "\n",
        "        # Extracting details using a loop through all label elements\n",
        "        for label_element in school_details_div.find_all('label'):\n",
        "            label_text = label_element.text.strip().rstrip(':')\n",
        "            # Extracting corresponding detail text\n",
        "            detail_text = label_element.find_next('div', class_='edu-school-det-text').text.strip() if label_element else None\n",
        "            details[label_text] = detail_text\n",
        "\n",
        "        return details\n",
        "    else:\n",
        "        print(\"School details not found.\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape all schools from multiple pages\n",
        "def scrape_all_schools(base_url, num_pages):\n",
        "    # Setting headers to simulate a user-agent\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0'}\n",
        "\n",
        "    # Initializing a list to store all schools' data\n",
        "    all_schools_data = []\n",
        "\n",
        "    # Looping through a range of page numbers\n",
        "    for page_number in range(1, num_pages + 1):\n",
        "        # Constructing the URL for the current page\n",
        "        url = f\"{base_url}?page={page_number}\"\n",
        "\n",
        "        # Making a GET request to the current page\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        # Checking if the request was successful (status code 200)\n",
        "        if response.status_code == 200:\n",
        "            # Parsing the HTML content using BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Finding the parent element containing all the li elements\n",
        "            parent_element = soup.find('div', class_='edu-school-list-wrap')\n",
        "\n",
        "            # Checking if the parent element is found\n",
        "            if parent_element:\n",
        "                # Finding all li elements within the parent element\n",
        "                li_elements = parent_element.find_all('li')\n",
        "\n",
        "                # Iterating through each li element and extracting school data\n",
        "                for li_element in li_elements:\n",
        "                    school_data = scrape_school_details(li_element)\n",
        "                    if school_data:\n",
        "                        all_schools_data.append(school_data)\n",
        "\n",
        "            else:\n",
        "                print(f\"Parent element not found on page {page_number}.\")\n",
        "\n",
        "        # Handling the case where the page is not found (status code 404)\n",
        "        elif response.status_code == 404:\n",
        "            print(f\"Page not found. Status code: {response.status_code} for page {page_number}\")\n",
        "\n",
        "    return all_schools_data\n",
        "\n",
        "# Function to save scraped school data to an Excel file\n",
        "def save_to_excel(data, output_file='all_schools_details0.xlsx'):\n",
        "    # Extracting column names from the keys of the first dictionary in the data list\n",
        "    columns = list(data[0].keys())\n",
        "\n",
        "    # Creating a DataFrame from the data\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "    # Saving the DataFrame to an Excel file without an index column\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "# Main execution section\n",
        "if __name__ == \"__main__\":\n",
        "    # Defining the base URL for scraping CBSE schools in foreign countries\n",
        "    base_url = \"https://www.careerindia.com/cbse-schools-in-foreign-schools-s11.html\"\n",
        "\n",
        "    # Specifying the number of pages to scrape (assuming 25 pages)\n",
        "    num_pages = 24  # Assuming you have 25 pages\n",
        "\n",
        "    # Initializing an empty list to store scraped data from all pages\n",
        "    all_schools_data = []\n",
        "\n",
        "    # Looping through a range of page numbers\n",
        "    for page_number in range(1, num_pages + 1):\n",
        "        # Calling the scrape_all_schools function to scrape data from the current page\n",
        "        page_data = scrape_all_schools(base_url, page_number)\n",
        "\n",
        "        # Checking if data was successfully scraped from the page\n",
        "        if page_data:\n",
        "            # Extending the list with data from the current page\n",
        "            all_schools_data.extend(page_data)\n",
        "\n",
        "    # Checking if any school data was collected\n",
        "    if all_schools_data:\n",
        "        # Calling the save_to_excel function to export the data to an Excel file\n",
        "        save_to_excel(all_schools_data)\n",
        "    else:\n",
        "        print(\"No school data found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHRvsl3SyT5s",
        "outputId": "461fac41-881b-4458-e45f-9c6675196ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to all_schools_details0.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMGwGON71kkK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}