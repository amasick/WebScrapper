{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FRsVqIfDjSMA-jUFkCrx2mrEoW4bK1vv",
      "authorship_tag": "ABX9TyPelUHDmQGeyUjjk99AjN9h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amasick/WebScrapper/blob/main/Schools_Details.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tamil Nadu Schools**"
      ],
      "metadata": {
        "id": "8n6AJx81CdcZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OgRLcG6hPyA",
        "outputId": "c922eeb5-25d3-4869-ee0c-12a80100ce90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to Outside_India.xlsx\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "def scrape_school_details(catbox_div):\n",
        "    school_name = catbox_div.find('h2').find('a').text.strip()\n",
        "\n",
        "    # Find the paragraph containing school details\n",
        "    school_details_paragraph = catbox_div.find('p')\n",
        "\n",
        "    # Check if the paragraph is found before accessing its content\n",
        "    if school_details_paragraph:\n",
        "        # Use regex to extract email\n",
        "        email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', school_details_paragraph.text)\n",
        "        email = email_match.group(0).strip() if email_match else None\n",
        "\n",
        "\n",
        "        # Use regex to extract address\n",
        "        address_match = re.search(r'Address of the school is:(.*?)PIN Code:', school_details_paragraph.text)\n",
        "        address = address_match.group(1).strip() if address_match else None\n",
        "        # print(address)\n",
        "\n",
        "        # Use regex to extract PIN code\n",
        "        pin_code_match = re.search(r'PIN Code:\\s*([^<.]+)', school_details_paragraph.text)\n",
        "        pin_code = pin_code_match.group(1).strip() if pin_code_match else None\n",
        "\n",
        "        # Use regex to extract management\n",
        "        management_match = re.search(r'The school is being managed by\\s*([^<]+)\\.', school_details_paragraph.text)\n",
        "        management = management_match.group(1).strip() if management_match else None\n",
        "\n",
        "        read_more_link = school_details_paragraph.find('a', class_='link')['href']\n",
        "\n",
        "        return {\n",
        "            'School Name': school_name,\n",
        "            'Email': email,\n",
        "            'Address': address,\n",
        "            'PIN Code': pin_code,\n",
        "            'Management': management,\n",
        "            'Read More Link': read_more_link\n",
        "        }\n",
        "    else:\n",
        "        print(\"School details paragraph not found.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def scrape_all_schools(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0'}\n",
        "\n",
        "    # Make the GET request with custom headers\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        catbox_divs = soup.find_all('div', class_='catbox')\n",
        "\n",
        "        all_schools_data = []\n",
        "        for catbox_div in catbox_divs:\n",
        "            school_data = scrape_school_details(catbox_div)\n",
        "            all_schools_data.append(school_data)\n",
        "\n",
        "        return all_schools_data\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
        "        return None\n",
        "def save_to_excel(data, output_file='Outside_India.xlsx'):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://www.cbseschool.org/location/outside-india/page/\"\n",
        "    all_schools_data = []\n",
        "\n",
        "    for page_number in range(1,5):  # Assuming you have 24 pages\n",
        "        url = f\"{base_url}{page_number}\"\n",
        "        page_schools_data = scrape_all_schools(url)\n",
        "\n",
        "        if page_schools_data:\n",
        "            all_schools_data.extend(page_schools_data)\n",
        "\n",
        "    if all_schools_data:\n",
        "        save_to_excel(all_schools_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **After Read More links**\n"
      ],
      "metadata": {
        "id": "NUTeVuSj9gF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_school_details(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the div containing school details\n",
        "        details_div = soup.find('div', id='schooldetails')\n",
        "\n",
        "        if details_div:\n",
        "            # Extracting school name from table heading\n",
        "            school_name = details_div.find('tr', class_='tableheading').find_all('td')[1].text.strip()\n",
        "            # Extracting the values using a loop through all td elements\n",
        "            details = {'School Name': school_name}\n",
        "            for td_element in details_div.find_all('td', class_='field'):\n",
        "                field_name = td_element.text.strip()\n",
        "                field_value = td_element.find_next('td').text.strip() if td_element else None\n",
        "                details[field_name] = field_value\n",
        "\n",
        "            return details\n",
        "        else:\n",
        "            print(\"School details not found.\")\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.cbseschool.org/a-a-public-school-tamil-nadu/\"\n",
        "    school_data = scrape_school_details(url)\n",
        "\n",
        "    if school_data:\n",
        "        print(school_data)\n",
        "    else:\n",
        "        print(\"Failed to scrape school details.\")\n"
      ],
      "metadata": {
        "id": "kBS61Xx6dMBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772c0eb7-0ae8-4977-aa99-1077e9a662c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'School Name': 'A A Public School', 'Affiliate ID': '1931043', 'Address': '17/8, Andiappan Gramani Street, Royapuram, Chennai', 'PIN Code': '600013', 'Office Phone': '04448595902', 'E-mail': 'aapublicschool@gmail.com', 'Foundation Year': '2017', 'Principal/Head of Institution': 'Mrs. S.gnana Jothi', 'School Status': 'Secondary School', 'Managing Trust/Society/Committee': 'Aladi Aruna Foundation'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_school_urls(base_url, num_pages):\n",
        "    all_school_urls = []\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'}\n",
        "\n",
        "    for page_number in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}/page/{page_number}/\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all catbox divs\n",
        "            catbox_divs = soup.find_all('div', class_='catbox')\n",
        "\n",
        "            # Extract URLs from h2 tags\n",
        "            page_school_urls = [catbox.find('h2').find('a')['href'] for catbox in catbox_divs]\n",
        "\n",
        "            all_school_urls.extend(page_school_urls)\n",
        "        else:\n",
        "            print(f\"Failed to fetch data from {url}. Status code: {response.status_code}\")\n",
        "\n",
        "    return all_school_urls\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://www.cbseschool.org/location/tamil-nadu\"\n",
        "    num_pages = 25\n",
        "\n",
        "    school_urls = get_school_urls(base_url, num_pages)\n",
        "\n",
        "    if school_urls:\n",
        "        print(\"List of School URLs:\")\n",
        "        for school_url in school_urls:\n",
        "            print(school_url)\n",
        "    else:\n",
        "        print(\"No URLs found.\")\n"
      ],
      "metadata": {
        "id": "EcI9_7kXErG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_all_schools(school_urls):\n",
        "    all_school_data = []\n",
        "\n",
        "    for url in school_urls:\n",
        "        school_data = scrape_school_details(url)\n",
        "        if school_data:\n",
        "            all_school_data.append(school_data)\n",
        "\n",
        "    return all_school_data"
      ],
      "metadata": {
        "id": "c4ALNjnUFjmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://www.cbseschool.org/location/tamil-nadu\"\n",
        "    num_pages = 25\n",
        "\n",
        "    school_urls = []\n",
        "    for page_number in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}/page/{page_number}/\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            catbox_divs = soup.find_all('div', class_='catbox')\n",
        "            page_school_urls = [catbox.find('h2').find('a')['href'] for catbox in catbox_divs]\n",
        "            school_urls.extend(page_school_urls)\n",
        "        else:\n",
        "            print(f\"Failed to fetch data from {url}. Status code: {response.status_code}\")\n",
        "\n",
        "    all_school_data = scrape_all_schools(school_urls)\n",
        "\n",
        "    if all_school_data:\n",
        "        for school_data in all_school_data:\n",
        "            print(school_data)\n",
        "    else:\n",
        "        print(\"No school data found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_BsSChDF2v8",
        "outputId": "6876b394-141a-4c70-9e78-888f625a5acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/1/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/2/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/3/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/4/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/5/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/6/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/7/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/8/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/9/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/10/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/11/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/12/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/13/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/14/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/15/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/16/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/17/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/18/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/19/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/20/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/21/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/22/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/23/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/24/. Status code: 406\n",
            "Failed to fetch data from https://www.cbseschool.org/location/tamil-nadu/page/25/. Status code: 406\n",
            "No school data found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_school_urls(base_url, num_pages):\n",
        "    all_school_urls = []\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'}\n",
        "\n",
        "    for page_number in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}/page/{page_number}/\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all catbox divs\n",
        "            catbox_divs = soup.find_all('div', class_='catbox')\n",
        "\n",
        "            # Extract URLs from h2 tags\n",
        "            page_school_urls = [catbox.find('h2').find('a')['href'] for catbox in catbox_divs]\n",
        "\n",
        "            all_school_urls.extend(page_school_urls)\n",
        "        else:\n",
        "            print(f\"Failed to fetch data from {url}. Status code: {response.status_code}\")\n",
        "\n",
        "    return all_school_urls\n",
        "\n",
        "def scrape_school_details(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the div containing school details\n",
        "        details_div = soup.find('div', id='schooldetails')\n",
        "\n",
        "        if details_div:\n",
        "            # Extracting school name from table heading\n",
        "            school_name = details_div.find('tr', class_='tableheading').find_all('td')[1].text.strip()\n",
        "            # Extracting the values using a loop through all td elements\n",
        "            details = {'School Name': school_name}\n",
        "            for td_element in details_div.find_all('td', class_='field'):\n",
        "                field_name = td_element.text.strip()\n",
        "                field_value = td_element.find_next('td').text.strip() if td_element else None\n",
        "                details[field_name] = field_value\n",
        "\n",
        "            return details\n",
        "        else:\n",
        "            print(\"School details not found.\")\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def scrape_all_schools(school_urls):\n",
        "    all_school_data = []\n",
        "\n",
        "    for url in school_urls:\n",
        "        school_data = scrape_school_details(url)\n",
        "        if school_data:\n",
        "            all_school_data.append(school_data)\n",
        "\n",
        "    return all_school_data\n",
        "\n",
        "def save_to_excel(data, output_file='TN_DetailedSchool_details.xlsx'):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://www.cbseschool.org/location/tamil-nadu\"\n",
        "    num_pages = 24\n",
        "\n",
        "    school_urls = get_school_urls(base_url, num_pages)\n",
        "\n",
        "    if school_urls:\n",
        "        all_school_data = scrape_all_schools(school_urls)\n",
        "\n",
        "        if all_school_data:\n",
        "            save_to_excel(all_school_data)\n",
        "        else:\n",
        "            print(\"No school data found.\")\n",
        "    else:\n",
        "        print(\"No URLs found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_kyxeX3JqFR",
        "outputId": "68416fd2-ed77-4571-8a5c-895022bf7cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to TN_DetailedSchool_details.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_school_urls(base_url, num_pages):\n",
        "    all_school_urls = []\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'}\n",
        "\n",
        "    for page_number in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}/page/{page_number}/\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all catbox divs\n",
        "            catbox_divs = soup.find_all('div', class_='catbox')\n",
        "\n",
        "            # Extract URLs from h2 tags\n",
        "            page_school_urls = [catbox.find('h2').find('a')['href'] for catbox in catbox_divs]\n",
        "\n",
        "            all_school_urls.extend(page_school_urls)\n",
        "        else:\n",
        "            print(f\"Failed to fetch data from {url}. Status code: {response.status_code}\")\n",
        "\n",
        "    return all_school_urls\n",
        "\n",
        "def scrape_school_details(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the div containing school details\n",
        "        details_div = soup.find('div', id='schooldetails')\n",
        "\n",
        "        if details_div:\n",
        "            # Extracting school name from table heading\n",
        "            school_name = details_div.find('tr', class_='tableheading').find_all('td')[1].text.strip()\n",
        "            # Extracting the values using a loop through all td elements\n",
        "            details = {'School Name': school_name}\n",
        "            for td_element in details_div.find_all('td', class_='field'):\n",
        "                field_name = td_element.text.strip()\n",
        "                field_value = td_element.find_next('td').text.strip() if td_element else None\n",
        "                details[field_name] = field_value\n",
        "\n",
        "            return details\n",
        "        else:\n",
        "            print(\"School details not found.\")\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def scrape_all_schools(school_urls):\n",
        "    all_school_data = []\n",
        "\n",
        "    for url in school_urls:\n",
        "        school_data = scrape_school_details(url)\n",
        "        if school_data:\n",
        "            all_school_data.append(school_data)\n",
        "\n",
        "    return all_school_data\n",
        "\n",
        "def save_to_excel(data, output_file='OutsideIndiaSchool_details.xlsx'):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://www.cbseschool.org/location/outside-india/\"\n",
        "    num_pages = 5\n",
        "\n",
        "    school_urls = get_school_urls(base_url, num_pages)\n",
        "\n",
        "    if school_urls:\n",
        "        all_school_data = scrape_all_schools(school_urls)\n",
        "\n",
        "        if all_school_data:\n",
        "            save_to_excel(all_school_data)\n",
        "        else:\n",
        "            print(\"No school data found.\")\n",
        "    else:\n",
        "        print(\"No URLs found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzIz2kyjLitT",
        "outputId": "71587441-95a3-4028-d80d-950d4becc256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to OutsideIndiaSchool_details.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Foriegn Schools**"
      ],
      "metadata": {
        "id": "BSq083Rai7fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "Hh2kr87HhqIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_school_details(li_element):\n",
        "    # Find the div containing school details\n",
        "    school_details_div = li_element.find('div', class_='edu-school-detlist-container')\n",
        "\n",
        "    if school_details_div:\n",
        "        # Extracting the school name\n",
        "        school_name = school_details_div.find('h2', class_='edu-school-det-heading').text.strip()\n",
        "        print(\"School Name:\", school_name)\n",
        "\n",
        "        # Extracting the values using a loop through all label elements\n",
        "        details = {'School Name': school_name}\n",
        "        for label_element in school_details_div.find_all('label'):\n",
        "            label_text = label_element.text.strip()\n",
        "            detail_text = label_element.find_next('div', class_='edu-school-det-text').text.strip() if label_element else None\n",
        "            details[label_text] = detail_text\n",
        "            print(f\"{label_text}: {detail_text}\")\n",
        "\n",
        "        return details\n",
        "    else:\n",
        "        print(\"School details not found.\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "Qjt7ECQmk1om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_all_schools(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0'}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the parent element containing all the li elements\n",
        "        parent_element = soup.find('div', class_='edu-school-list-wrap')\n",
        "\n",
        "        if parent_element:\n",
        "            # Find all li elements within the parent element\n",
        "            li_elements = parent_element.find_all('li')\n",
        "\n",
        "            all_schools_data = []\n",
        "            for li_element in li_elements:\n",
        "                school_data = scrape_school_details(li_element)\n",
        "                all_schools_data.append(school_data)\n",
        "\n",
        "            return all_schools_data  # Corrected indentation\n",
        "\n",
        "        else:\n",
        "            print(\"Parent element not found.\")\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "79QLH22Ak9vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_excel(data, output_file='all_schools_details133.xlsx'):\n",
        "    # Adjust column names based on the provided data\n",
        "    # columns = ['School Name', 'Address', 'Pincode', 'Contact', 'Email', 'Website']\n",
        "\n",
        "    # Create a DataFrame from the data\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.careerindia.com/cbse-schools-in-foreign-schools-s11.html\"\n",
        "    all_schools_data = scrape_all_schools(url)\n",
        "\n",
        "    # # Assuming you have 24 pages\n",
        "    # for page_number in range(1, 25):\n",
        "    #     url = f\"{base_url}{page_number}\"\n",
        "        # page_schools_data = scrape_all_schools(url)\n",
        "\n",
        "        # if page_schools_data:\n",
        "        #     all_schools_data.extend(page_schools_data)\n",
        "\n",
        "    if all_schools_data:\n",
        "        save_to_excel(all_schools_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l12v-xJptwW6",
        "outputId": "1cabbd5f-d855-43f6-d76b-db1002f6fde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to all_schools_details133.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_excel(data, output_file='all_schools_details13.xlsx'):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")"
      ],
      "metadata": {
        "id": "IXRYc9p0lCJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.careerindia.com/cbse-schools-in-foreign-schools-s11.html\"\n",
        "    all_schools_data = []\n",
        "\n",
        "\n",
        "\n",
        "    page_schools_data = scrape_all_schools(url)\n",
        "\n",
        "    if page_schools_data:\n",
        "        all_schools_data.extend(page_schools_data)\n",
        "\n",
        "    if all_schools_data:\n",
        "        save_to_excel(all_schools_data)\n"
      ],
      "metadata": {
        "id": "MSj3GnOWlGMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Outside India Schools**"
      ],
      "metadata": {
        "id": "0ZVXL8xS-CAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "def scrape_school_details(catbox_div):\n",
        "    school_name = catbox_div.find('h2').find('a').text.strip()\n",
        "\n",
        "    # Find the paragraph containing school details\n",
        "    school_details_paragraph = catbox_div.find('p')\n",
        "\n",
        "    # Check if the paragraph is found before accessing its content\n",
        "    if school_details_paragraph:\n",
        "        # Use regex to extract email\n",
        "        email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', school_details_paragraph.text)\n",
        "        email = email_match.group(0).strip() if email_match else None\n",
        "\n",
        "\n",
        "        # Use regex to extract address\n",
        "        address_match = re.search(r'Address of the school is:(.*?)PIN Code:', school_details_paragraph.text)\n",
        "        address = address_match.group(1).strip() if address_match else None\n",
        "        # print(address)\n",
        "\n",
        "        # Use regex to extract PIN code\n",
        "        pin_code_match = re.search(r'PIN Code:\\s*([^<.]+)', school_details_paragraph.text)\n",
        "        pin_code = pin_code_match.group(1).strip() if pin_code_match else None\n",
        "\n",
        "        # Use regex to extract management\n",
        "        management_match = re.search(r'The school is being managed by\\s*([^<]+)\\.', school_details_paragraph.text)\n",
        "        management = management_match.group(1).strip() if management_match else None\n",
        "\n",
        "        read_more_link = school_details_paragraph.find('a', class_='link')['href']\n",
        "\n",
        "        return {\n",
        "            'School Name': school_name,\n",
        "            'Email': email,\n",
        "            'Address': address,\n",
        "            'PIN Code': pin_code,\n",
        "            'Management': management,\n",
        "            'Read More Link': read_more_link\n",
        "        }\n",
        "    else:\n",
        "        print(\"School details paragraph not found.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def scrape_all_schools(url):\n",
        "    # headers = {\n",
        "    #     'User-Agent': 'Thunder Client (https://www.thunderclient.com)',\n",
        "    #     'Accept': '/'\n",
        "    # }\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0'}\n",
        "\n",
        "\n",
        "    # Make the GET request with custom headers\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        catbox_divs = soup.find_all('div', class_='catbox')\n",
        "\n",
        "        all_schools_data = []\n",
        "        for catbox_div in catbox_divs:\n",
        "            school_data = scrape_school_details(catbox_div)\n",
        "            all_schools_data.append(school_data)\n",
        "\n",
        "        return all_schools_data\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def save_to_excel(data, output_file='all_schools_details12.xlsx'):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://www.cbseschool.org/location/outside-india/\"\n",
        "    all_schools_data = []\n",
        "\n",
        "    for page_number in range(1, 25):  # Assuming you have 24 pages\n",
        "        url = f\"{base_url}{page_number}\"\n",
        "        page_schools_data = scrape_all_schools(url)\n",
        "\n",
        "        if page_schools_data:\n",
        "            all_schools_data.extend(page_schools_data)\n",
        "\n",
        "    if all_schools_data:\n",
        "        save_to_excel(all_schools_data)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qguXOLRB-NZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_school_details(li_element):\n",
        "    # Find the div containing school details\n",
        "    school_details_div = li_element.find('div', class_='edu-school-detlist-container')\n",
        "\n",
        "    if school_details_div:\n",
        "        # Extracting the school name\n",
        "        school_name = school_details_div.find('h2', class_='edu-school-det-heading').text.strip()\n",
        "\n",
        "        # Extracting the values using a loop through all label elements\n",
        "        details = {'School Name': school_name}\n",
        "        for label_element in school_details_div.find_all('label'):\n",
        "            label_text = label_element.text.strip().rstrip(':')\n",
        "            detail_text = label_element.find_next('div', class_='edu-school-det-text').text.strip() if label_element else None\n",
        "            details[label_text] = detail_text\n",
        "\n",
        "        return details\n",
        "    else:\n",
        "        print(\"School details not found.\")\n",
        "        return None\n",
        "\n",
        "def scrape_all_schools(base_url, num_pages):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0'}\n",
        "\n",
        "    all_schools_data = []\n",
        "    for page_number in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}?page={page_number}\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find the parent element containing all the li elements\n",
        "            parent_element = soup.find('div', class_='edu-school-list-wrap')\n",
        "\n",
        "            if parent_element:\n",
        "                # Find all li elements within the parent element\n",
        "                li_elements = parent_element.find_all('li')\n",
        "\n",
        "                for li_element in li_elements:\n",
        "                    school_data = scrape_school_details(li_element)\n",
        "                    if school_data:\n",
        "                        all_schools_data.append(school_data)\n",
        "\n",
        "            else:\n",
        "                print(f\"Parent element not found on page {page_number}.\")\n",
        "\n",
        "        elif response.status_code == 404:\n",
        "            print(f\"Page not found. Status code: {response.status_code} for page {page_number}\")\n",
        "\n",
        "    return all_schools_data\n",
        "\n",
        "def save_to_excel(data, output_file='all_schools_details0.xlsx'):\n",
        "    # Extract column names from the keys of the first dictionary in the data list\n",
        "    columns = list(data[0].keys())\n",
        "\n",
        "    # Create a DataFrame from the data\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://www.careerindia.com/cbse-schools-in-foreign-schools-s11.html\"\n",
        "    num_pages = 24  # Assuming you have 25 pages\n",
        "\n",
        "    all_schools_data = []\n",
        "    for page_number in range(1, num_pages + 1):\n",
        "        page_data = scrape_all_schools(base_url, page_number)\n",
        "        if page_data:\n",
        "            all_schools_data.extend(page_data)\n",
        "\n",
        "    if all_schools_data:\n",
        "        save_to_excel(all_schools_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHRvsl3SyT5s",
        "outputId": "461fac41-881b-4458-e45f-9c6675196ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to all_schools_details0.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMGwGON71kkK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}